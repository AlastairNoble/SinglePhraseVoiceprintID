{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers = os.listdir('alexa/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alexa/anfcucvo/1.wav</td>\n",
       "      <td>anfcucvo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alexa/anfcucvo/2.wav</td>\n",
       "      <td>anfcucvo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alexa/anfcucvo/3.wav</td>\n",
       "      <td>anfcucvo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alexa/anfcucvo/4.wav</td>\n",
       "      <td>anfcucvo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alexa/anfcucvo/5.wav</td>\n",
       "      <td>anfcucvo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               filepath   speaker\n",
       "0  alexa/anfcucvo/1.wav  anfcucvo\n",
       "1  alexa/anfcucvo/2.wav  anfcucvo\n",
       "2  alexa/anfcucvo/3.wav  anfcucvo\n",
       "3  alexa/anfcucvo/4.wav  anfcucvo\n",
       "4  alexa/anfcucvo/5.wav  anfcucvo"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['filepath', 'speaker'])\n",
    "for speaker in speakers:\n",
    "    files = os.listdir('alexa/{}/'.format(speaker))\n",
    "    for file in files:\n",
    "        filepath = 'alexa/{}/{}'.format(speaker, file)\n",
    "        df = df.append({'filepath':filepath, 'speaker':speaker}, ignore_index=True)\n",
    "\n",
    "df.head() # this is just a print statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>alexa/kxrnhrcj/2.wav</td>\n",
       "      <td>kxrnhrcj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>alexa/kebwpdyu/4.wav</td>\n",
       "      <td>kebwpdyu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>alexa/kwavzzrt/1.wav</td>\n",
       "      <td>kwavzzrt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>alexa/kpkwyaut/3.wav</td>\n",
       "      <td>kpkwyaut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>alexa/zgmrhuwb/3.wav</td>\n",
       "      <td>zgmrhuwb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 filepath   speaker\n",
       "178  alexa/kxrnhrcj/2.wav  kxrnhrcj\n",
       "142  alexa/kebwpdyu/4.wav  kebwpdyu\n",
       "163  alexa/kwavzzrt/1.wav  kwavzzrt\n",
       "157  alexa/kpkwyaut/3.wav  kpkwyaut\n",
       "359  alexa/zgmrhuwb/3.wav  zgmrhuwb"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = train_test_split(df, test_size = 0.25, stratify = df['speaker'])\n",
    "# the stratify parameter makes the function split data evenly over the speakers column\n",
    "# this is so we dont get all files of the same speaker in the test set and not the training set\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(filename):\n",
    "    \n",
    "    X, sample_rate = librosa.load(filename, res_type='kaiser_fast')\n",
    "    \n",
    "    mfccs = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40)\n",
    "\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.9909015e+02 -3.6271417e+02 -2.7202203e+02 ... -3.6679407e+02\n",
      "  -3.6198358e+02 -3.6131876e+02]\n",
      " [ 0.0000000e+00  3.8621197e+01  6.0143829e+01 ...  1.4929198e+02\n",
      "   1.5024278e+02  1.4371318e+02]\n",
      " [ 0.0000000e+00 -1.2240431e+01 -3.0983868e+01 ...  1.6902367e+01\n",
      "   1.8225193e+01  1.4989624e+01]\n",
      " ...\n",
      " [ 0.0000000e+00 -4.5158615e+00 -2.9134605e+00 ... -3.1189102e-01\n",
      "   4.3945408e-01  2.1566045e+00]\n",
      " [ 0.0000000e+00  6.4124022e+00  4.1545420e+00 ...  2.9092536e+00\n",
      "   1.2940142e+00  1.5603128e-01]\n",
      " [ 0.0000000e+00  8.1574726e+00  8.6703691e+00 ... -4.1870975e-01\n",
      "  -1.6172510e+00 -1.2464466e+00]]\n"
     ]
    }
   ],
   "source": [
    "# test out the function on one of the files\n",
    "print(extract_features(train['filepath'][178]))\n",
    "# it returns an array of 40 arrays so we need to flatten it into a 1d array later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178    [[-599.09015, -362.71417, -272.02203, -277.916...\n",
       "142    [[-675.10297, -675.10297, -663.4995, -659.2624...\n",
       "163    [[-327.3413, -336.8754, -435.57803, -499.06454...\n",
       "157    [[-352.7316, -367.68524, -400.95602, -431.2065...\n",
       "359    [[-534.69324, -524.5771, -519.0945, -518.5205,...\n",
       "Name: filepath, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the extract features to every element in train \n",
    "train_features = train['filepath'].apply(extract_features)\n",
    "# train_features is now an array of 2d arrays\n",
    "# each 2d array is the features of one audio file\n",
    "test_features = test['filepath'].apply(extract_features)\n",
    "train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 116)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[272].shape\n",
    "#40 arrays, each with 116 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now to flatten each 2d array into a 1d array\n",
    "train_features_1d = []\n",
    "for features in train_features:\n",
    "    features_1d = features.flatten()\n",
    "    train_features_1d.append(features_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.9909015e+02 -3.6271417e+02 -2.7202203e+02 ... -4.1870975e-01\n",
      " -1.6172510e+00 -1.2464466e+00]\n"
     ]
    }
   ],
   "source": [
    "print(train_features_1d[0]) # now this is a single array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same thing for the test features\n",
    "test_features_1d = []\n",
    "for features in test_features:\n",
    "    features_1d = features.flatten()\n",
    "    test_features_1d.append(features_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into X and Y where X is the features and Y is the label (name of speaker)\n",
    "# remember that each array is still in the same order as above \n",
    "# so each element in X_train corresponds to an element in Y_train at the same index\n",
    "X_train = np.asarray(train_features_1d)\n",
    "X_test = np.array(test_features_1d)\n",
    "Y_train = np.array(train['speaker'])\n",
    "Y_test = np.array(test['speaker'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker: kxrnhrcj\n",
      "Features: [-5.9909015e+02 -3.6271417e+02 -2.7202203e+02 ... -4.1870975e-01\n",
      " -1.6172510e+00 -1.2464466e+00]: \n"
     ]
    }
   ],
   "source": [
    "# now X_train is a 2d array, and each array is the long array of mfccs\n",
    "print(\"speaker: {}\".format(Y_train[0]))\n",
    "print(\"Features: {}: \".format(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
